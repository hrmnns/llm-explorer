{
    "0": {
        "title": "Tokenisierung",
        "subtitle": "Vom Wort zum Baustein",
        "mission": "Hier zerlegt die KI deinen Satz in 'Tokens'. Da Computer nur Zahlen verstehen, ist dies der erste Schritt der Abstraktion. Ein Wort kann ein Token sein, aber auch nur ein Teil eines Wortes.",
        "steps": [
            "Klicke auf die Tokens, um ihre eindeutige ID in der Datenbank zu sehen.",
            "Beobachte, wie Satzzeichen oder Wortteile oft eigene Tokens sind.",
            "Pr√ºfe im Inspektor (rechts), wie die statistische Verteilung der Tokens aussieht."
        ],
        "insight": "Tokens sind die 'Atome' der Sprache f√ºr ein Sprachmodell ‚Äì ohne Semantik, nur als Index.",
        "externalResource": {
            "label": "Interaktiver OpenAI Tokenizer",
            "url": "https://platform.openai.com/tokenizer"
        }
    },
    "1": {
        "title": "Embedding",
        "subtitle": "Sprache wird Mathematik",
        "mission": "Tokens werden in hochdimensionale Vektoren umgewandelt. Jedes Wort bekommt eine Koordinate in einem abstrakten Bedeutungsraum. Hier entscheiden die 'Achsen', welche Merkmale wir betrachten.",
        "steps": [
            "Nutze die konfigurierbaren Achsen im Koordinatensystem, um W√∂rter nach verschiedenen Eigenschaften zu sortieren.",
            "Erkenne, wie semantische N√§he (z.B. 'Hund' und 'Welpe') in mathematische N√§he √ºbersetzt wird.",
            "Achte auf die Dimensionen ‚Äì jedes Feld im Vektor steht f√ºr eine abstrakte, gelernte Eigenschaft."
        ],
        "insight": "Im Vektorraum haben W√∂rter 'Nachbarn'. Die Achsen bestimmen, ob 'Schloss' eher bei 'T√ºr' oder bei 'Burg' liegt.",
        "externalResource": {
            "label": "TensorFlow Embedding Projector",
            "url": "https://projector.tensorflow.org/"
        }
    },
    "2": {
        "title": "Multi-Head Attention",
        "subtitle": "Der Kontext-Selektor",
        "mission": "Hier passiert die Magie: Die KI stellt 'Fragen' (Queries) an den Satz. Verschiedene 'Heads' (K√∂pfe) analysieren gleichzeitig Grammatik, Logik oder soziale Rollen, um Mehrdeutigkeiten (Polysemie) aufzul√∂sen.",
        "steps": [
            "W√§hle Head 3 (Logik), um zu sehen, wie er bei Werten > 0.7 den Fokus auf funktionale Aspekte lenkt.",
            "Schalte zwischen Layern um: Beobachte, wie der 'Einbrecher' im Satz die Bedeutung von 'Schloss' massiv verschiebt.",
            "Dicke, leuchtende Linien zeigen einen hohen 'Attention Score' ‚Äì hier flie√üt die meiste Information f√ºr die aktuelle Wortwahl."
        ],
        "insight": "Self-Attention ist ein dynamisches Filtersystem, das entscheidet, welche Information im aktuellen Kontext (z.B. 'Einbrecher' vs. 'K√∂nig') relevant ist.",
        "externalResource": {
            "label": "Deep Dive: The Illustrated Transformer",
            "url": "https://jalammar.github.io/illustrated-transformer/"
        }
    },
    "3": {
        "title": "Feed-Forward (FFN)",
        "subtitle": "Wissen & Kategorien aktivieren",
        "mission": "Nachdem der Kontext klar ist, wird das 'Weltwissen' aktiviert. Das Netzwerk gleicht die Informationen mit Kategorien wie 'Logisch', 'Sozial' oder 'Poetisch' ab. Eine Kategorie leuchtet gr√ºn, wenn sie stark aktiviert wird.",
        "steps": [
            "Beobachte die 'Live-Aktivierung' (A) der Kategorien. Sie ist das Resultat der Attention-Phase.",
            "Achte auf das 'Veredeln' der Vektoren: Hier wird entschieden, ob wir uns im Bereich 'Funktional' oder 'Prachtbau' bewegen.",
            "Siehst du eine starke Aktivierung bei 'Funktional'? Dann bereitet das Modell gerade eine technische Antwort vor."
        ],
        "insight": "Das FFN-Netzwerk fungiert als Wissensspeicher, der die Kontext-Signale in konkrete begriffliche Tendenzen umm√ºnzt.",
        "externalResource": {
            "label": "FFN als Key-Value Memory (Forschung)",
            "url": "https://ar5iv.labs.arxiv.org/html/2012.14913"
        }
    },
    "4": {
        "title": "Decoding & Softmax",
        "subtitle": "Die physikalische Wahrscheinlichkeit",
        "mission": "Die KI berechnet nun die Logit-Bias mit der Formel $B = (A - 0.5) \times 12$. Die Aktivierung aus Phase 3 verschiebt die Wahrscheinlichkeiten so, dass das passendste Wort gewinnt.",
        "steps": [
            "Schau dir die Top-Kandidaten an: Ein üéØ markiert den Sieger, ein ü•¥ deutet auf hohe Instabilit√§t hin.",
            "Ver√§ndere die 'Temperature': Hohe Werte erzeugen 'Jitter' (Zittern) in den Balken und machen die Wahl gewagter.",
            "Beobachte, wie ein Logit-Shift die Wahrscheinlichkeit f√ºr Worte wie 'T√ºrschloss' nach oben schnellen l√§sst."
        ],
        "insight": "KI ist eine Vorhersagemaschine. Durch Logit-Biasing wird aus statistischer Wahrscheinlichkeit eine kontextnahe Entscheidung.",
        "externalResource": {
            "label": "Hugging Face: Decoding Strategies",
            "url": "https://huggingface.co/blog/how-to-generate"
        }
    },
    "5": {
        "title": "Analyse & Output",
        "subtitle": "Die Kausalkette verstehen",
        "mission": "Die Reise ist beendet. Hier siehst du das Ergebnis und die Kausalkette: Von der Attention (Kontext) √ºber die FFN-Aktivierung (Wissen) bis zum finalen Wort-Bias.",
        "steps": [
            "Analysiere das Ergebnis: War der Fokus eher wissenschaftlich, sozial oder poetisch?",
            "Nutze den Inspektor, um zu sehen: H√§tte ein schw√§cherer Logik-Head (Phase 2) ein anderes Wort (z.B. 'Residenz') gewinnen lassen?",
            "Pr√ºfe, ob 'Halluzinationen' durch zu hohes Rauschen (Noise) in Phase 4 beg√ºnstigt wurden."
        ],
        "insight": "Jede Antwort ist kein Zufall, sondern eine mathematische Kaskade durch alle Schichten des Modells.",
        "externalResource": {
            "label": "Anthropic: Mapping the Mind of LLMs",
            "url": "https://www.anthropic.com/news/mapping-mind-language-model"
        }
    }
}